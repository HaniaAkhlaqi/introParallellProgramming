# openMP_sieve2.cpp 
The whole range of numbers from 2 to max is parallelized

1. Include OpenMP header:
   #include <omp.h> 

2. Remove the Posix threads-related code, including `pthread.h` and the `pthread_mutex_t` definitions, since OpenMP provides a simpler way to parallelize code without the need for explicit thread management.

3. Replace the `sieve` function with a parallelized version using OpenMP's `#pragma omp parallel for` directive. In this case, parallelize the outer loop that iterates through numbers in the Sieve of Eratosthenes algorithm mong multiple threads.

4. Additionally, we use `#pragma omp critical` to protect the `primes` vector from concurrent access when adding prime numbers to it.

5.  compile script: chmod +x script1.sh
    run script: ./script1.sh


# openMP_sieve2a.cpp
primarily parallelize the loops within the `main` function, as the main computation happens there. The first thread will calculate primes from 2 to `sqrt(max)`, and then we will use OpenMP to parallelize the loops to calculate primes from `sqrt(max) + 1` to `max`.

1. Included the OpenMP header (`#include <omp.h>`) to enable OpenMP parallelization.

2. **Replaced Posix Threads with OpenMP**: Removed the Posix thread-related code (`pthread.h` and related functions) as we will use OpenMP for parallelization.

3. **Modified Sieve Function**: Modified the `sieve` function to accept a `start` and `max` parameter to specify the range of numbers to calculate primes for.

4. **Parallelized Loops with OpenMP**: Inside the `main` function, used OpenMP to parallelize the loop that calculates primes from `sqrt(max) + 1` to `max`. We determine the start and end ranges for each thread based on the thread's ID and the total number of threads.

5. **Removed Explicit Thread Creation and Joining**: Removed the explicit thread creation and joining code, as OpenMP handles thread creation and management automatically.

6. **Added Critical Section**: Used `#pragma omp critical` to ensure that the `primes` vector is accessed in a thread-safe manner when adding prime numbers to it.


# openmp_sieve3.cpp

1. We retained the `std::queue` for tasks and the `std::vector` for marking numbers as prime.

2. In the main function, we used OpenMP's `#pragma omp parallel` directive to create a parallel region for task generation and processing.

3. Inside the parallel region, we use `#pragma omp for schedule(dynamic)` to parallelize the generation of tasks for numbers up to `sqrt(max)`. OpenMP dynamically distributes the loop iterations among available threads.

4. We added a critical section (`#pragma omp critical`) inside the loop to ensure that only one thread at a time enqueues tasks into the `taskQueue` to avoid data races.

5. The second parallel region is for task processing with a critical section for dequeuing tasks. OpenMP simplifies parallelization by specifying the number of threads and using directives such as #pragma omp parallel to parallelize code sections.OpenMP handles thread creation and management automatically. There's no explicit thread creation or joining.It abstracts away many of the details of thread management and synchronization.

6. The rest of the code, including printing prime numbers and measuring execution time, remains largely unchanged.


# openmp_game_of_life.cpp
1. **Strategy 3: Parallelize Initialization**:

The `init_random` function is parallelized using OpenMp's `#pragma omp parallel for` directive initializes the `previous` and `current` arrays. 
The `private(i, pos)`  clause ensures that each thread has its private copy of the loop counters `i` and `pos`, and the `shared(array1, array2)` clause specifies that the `array1` and `array2` arrays are shared among all threads. 
The `#pragma omp critical` directive ensures that only one thread at a time can update the arrays to avoid data races.

2. **Strategy 1:Parallelize Cell Updates**:
** Strategy 1:Parallelize Cell Updates**:
   Only the outermost loop is parallelized with the `#pragma omp parallel for` directive where each thread processing a subset of time steps. Each time step involves updating the state of each cell based on its neighbors.  
   
   The `private(i, j, nbrs)` clause ensures that each thread has its private copies of loop counters `i`, `j`, and `nbrs`, while the `shared(current, previous)` clause specifies that the `current` and `previous` arrays are shared among all threads. Each thread independently calculates the new state of cells based on their neighbors.

3. **Strategy 2: Parallelize Swap Operation**:
   After updating the state of all cells for a given time step, the `current` and `previous` arrays need to be swapped. This operation is parallelized using OpenMP. 
   
   Each thread is responsible for copying the data from the `current` array to the `swap` array. 
   The `private(i, j)` clause ensures that each thread has its private copies of loop counters `i` and `j`, while the `shared(current, previous, swap)` clause specifies that these arrays are shared among all threads. 
   
   Finally, the pointers `swap`, `current`, and `previous` are swapped to update the array references.



































































   for me:
   The code you've provided is an implementation of Conway's Game of Life with some parallelization using OpenMP. To ensure that parallelization improves performance without introducing bugs, you should follow best practices for using OpenMP. Here are some suggestions:

1. **Set the Number of Threads:** Use `omp_set_num_threads()` to control the number of threads that OpenMP will use. You can set it before any parallel regions to specify the desired number of threads. For example:

    ```cpp
    int num_threads = 4; // Set the number of threads as needed
    omp_set_num_threads(num_threads);
    ```

    Adjust the `num_threads` variable according to your system's capabilities.

2. **Avoid Race Conditions:** Race conditions can occur when multiple threads access and modify shared data simultaneously. In your code, you've already used `#pragma omp parallel for` to parallelize the loops, which is a good start. However, be careful when updating shared arrays. In particular, the `swap` operation should be synchronized to avoid race conditions.

    Instead of:

    ```cpp
    // Swap current array with the previous array
    swap = current;
    current = previous;
    previous = swap;
    ```

    You should use a temporary pointer and ensure that this operation is atomic:

    ```cpp
    int** temp = current;
    current = previous;
    previous = temp;
    ```

3. **Use Proper Synchronization:** If you encounter any critical sections where multiple threads access and modify shared data, use `#pragma omp critical` as you've already done in `init_random`. This ensures that only one thread enters the critical section at a time.

4. **Avoid False Sharing:** Be aware of false sharing, which occurs when multiple threads access variables that share the same cache line. It can lead to performance degradation. Ensure that variables accessed by different threads are not close together in memory.

5. **Verify Results:** When parallelizing code, especially with optimizations like loop parallelization, make sure that the parallelized code produces the correct results. Verify that the parallel version of the Game of Life still follows the rules of the game.

6. **Profile and Tune:** Use profiling tools (e.g., `gprof`, `perf`, or vendor-specific tools) to identify performance bottlenecks and areas where parallelization can be further improved. Adjust the code accordingly.

7. **Error Handling:** Implement proper error handling for file operations, memory allocation, and other critical parts of your code.

8. **Clean Up:** Ensure that all allocated memory is properly deallocated to avoid memory leaks.

Remember that the effectiveness of parallelization depends on various factors, including the size of the problem, the number of available CPU cores, and the efficiency of the parallelization strategy. It's a good practice to measure the performance gains achieved by parallelization using different input sizes and thread counts to find the optimal configuration for your specific hardware and problem size.